---
title: "Log-Log model"
format:
  html:
    code-overflow: wrap
editor: visual
---

# Log_Log Linear Regression Specification and Reproducibility Protocol

```{r}
options(repos = c(CRAN = "https://cloud.r-project.org"))
```

The log–log linear regression model was developed to quantify the elasticity between copper cable theft incidents and their explanatory variables, capturing proportional rather than absolute changes.\

All continuous predictors and the dependent variable were log-transformed to stabilize variance and mitigate nonlinear scaling effects, ensuring that estimated coefficients represent elasticities—that is, the percentage change in theft cases associated with a one-percent change in the predictor.\

The modeling process began with a full specification that included all lagged variables previously found to be the most related to traffic copper cable theft, together with monthly dummy variables (February–December) to control for potential seasonality effects.\

Following the full specification, a series of nested models were iteratively estimated. Each iteration removed one non-significant predictor (e.g., `log_Scrap_Imports_CN.lag`, `log_Unwork_NSW.lag`, `Apr`, `Dec`) and re-evaluated the model’s overall performance using the Akaike Information Criterion (AIC) and adjusted R².\

This stepwise refinement process balances parsimony and explanatory power, ensuring that retained predictors are both statistically significant and theoretically meaningful.\

By progressively excluding redundant terms, the final model (`log_log_model_11`) achieves an optimal trade-off between model fit and interpretability.

```{r}
# Log-log linear regression model 
log_log_model_1 <- lm(log_cases ~ log_cases_diff + log_Total_theft.lag + log_Copper_price.lag + intervention.lag + log_Unwork_NSW.lag + log_Scrap_Imports_CN.lag + log_t_since.lag + Feb + Mar +  APr + May + Jun + Jul + Aug + Sep + Oct + Nov + Dec, data = train_data)

# Model summary
summary(log_log_model_1)
```

```{r}
# Log-log linear regression model (remove log_Scrap_Imports_CN.lag)
log_log_model_2 <- lm(log_cases ~ log_cases_diff + log_Total_theft.lag + log_Copper_price.lag + intervention.lag + log_Unwork_NSW.lag + log_t_since.lag + Mar +  APr + May + Jun + Jul + Aug + Sep + Oct + Nov + Dec, data = train_data)

# Model summary
summary(log_log_model_2)
```

```{r}
# Log-log linear regression model (remove log_Unwork_NSW.lag)
log_log_model_3 <- lm(log_cases ~ log_cases_diff + log_Total_theft.lag + log_Copper_price.lag + intervention.lag + log_t_since.lag + Mar +  APr + May + Jun + Jul + Aug + Sep + Oct + Nov + Dec, data = train_data)

# Model summary
summary(log_log_model_3)
```

```{r}
# Log-log linear regression model (remove log_cases_diff)
log_log_model_4 <- lm(log_cases ~ log_Total_theft.lag + log_Copper_price.lag + intervention.lag + log_t_since.lag + Mar +  APr + May + Jun + Jul + Aug + Sep + Oct + Nov + Dec, data = train_data)

# Model summary
summary(log_log_model_4)
```

```{r}
# Log-log linear regression model (remove APr)
log_log_model_5 <- lm(log_cases ~ log_Total_theft.lag + log_Copper_price.lag + intervention.lag + log_t_since.lag + Mar + May + Jun + Jul + Aug + Sep + Oct + Nov + Dec, data = train_data)

# Model summary
summary(log_log_model_5)
```

```{r}
# Log-log linear regression model (remove Dec)
log_log_model_6 <- lm(log_cases ~ log_Total_theft.lag + log_Copper_price.lag + intervention.lag + log_t_since.lag + Mar + May + Jun + Jul + Aug + Sep + Oct + Nov, data = train_data)

# Model summary
summary(log_log_model_6)
```

```{r}
# Log-log linear regression model (remove Nov)
log_log_model_7 <- lm(log_cases ~ log_Total_theft.lag + log_Copper_price.lag + intervention.lag + log_t_since.lag + Mar + May + Jun + Jul + Aug + Sep + Oct, data = train_data)

# Model summary
summary(log_log_model_7)
```

```{r}
# Log-log linear regression model (remove Jul)
log_log_model_8 <- lm(log_cases ~ log_Total_theft.lag + log_Copper_price.lag + intervention.lag + log_t_since.lag + Mar + May + Jun + Aug + Sep + Oct, data = train_data)

# Model summary
summary(log_log_model_8)
```

```{r}
# Log-log linear regression model (remove Mar)
log_log_model_9 <- lm(log_cases ~ log_Total_theft.lag + log_Copper_price.lag + intervention.lag + log_t_since.lag + May + Jun + Aug + Sep + Oct, data = train_data)

# Model summary
summary(log_log_model_9)
```

```{r}
# Log-log linear regression model (remove Oct)
log_log_model_10 <- lm(log_cases ~ log_Total_theft.lag + log_Copper_price.lag + intervention.lag + log_t_since.lag + May + Jun + Aug + Sep, data = train_data)

# Model summary
summary(log_log_model_10)
```

```{r}
# Log-log linear regression model (remove Aug)
log_log_model_10 <- lm(log_cases ~ log_Total_theft.lag + log_Copper_price.lag + intervention.lag + log_t_since.lag + May + Jun + Sep, data = train_data)

# Model summary
summary(log_log_model_11)
```

```{r}
# Log-log linear regression model (remove Aug)
log_log_model_11 <- lm(log_cases ~ log_Total_theft.lag + log_Copper_price.lag + intervention.lag + log_t_since.lag + May + Sep, data = train_data)

# Model summary
summary(log_log_model_11)
```

## Model Diagnostics

Comprehensive diagnostic tests were conducted on the residuals of the final model to validate the key assumptions of OLS regression.\

These included tests for normality (Shapiro–Wilk), homoscedasticity (Breusch–Pagan), autocorrelation (Durbin–Watson and Ljung–Box), and multicollinearity (Variance Inflation Factor).\

The purpose of these diagnostics is to ensure that model inference is reliable—violations of these assumptions can lead to biased or inefficient estimates.\

Additionally, visual residual analyses such as histograms, Q–Q plots, and residual-vs-fitted plots were generated to complement statistical tests, providing an intuitive assessment of model adequacy.

```{r}
# Extract residuals
residuals_log_log_model_11 <- residuals(log_log_model_11)

# Plot residual histogram
hist(residuals_log_log_model_11, breaks = 30, main = "Residuals Histogram", xlab = "Residuals", col = "blue")

# Generate Q–Q plot
qqnorm(residuals_log_log_model_11)
qqline(residuals_log_log_model_11, col = "red")

# Perform Shapiro–Wilk normality test 
shapiro.test(residuals_log_log_model_11)

# Plot residuals vs. fitted values
plot(fitted(log_log_model_11), residuals_log_log_model_11, main = "Residuals vs. Fitted",
     xlab = "Fitted values", ylab = "Residuals", pch = 20, col = "blue")
abline(h = 0, col = "red", lwd = 2)

# Breusch–Pagan test for heteroscedasticity
bptest(log_log_model_11)

# Autocorrelation tests
# Durbin–Watson test
dwtest(log_log_model_11)

# Ljung–Box test
Box.test(residuals_log_log_model_11, lag = 30, type = "Ljung-Box")

# Multicollinearity diagnostics
# Calculate VIF
vif(log_log_model_11)

library(tseries)
# ADF Test
adf.test(residuals_log_log_model_11)

# KPSS Test
kpss.test(residuals_log_log_model_11, null = "Level") 
```

## Out of Sample Prediction and Model Accuracy

Using the finalized model, out-of-sample forecasts were generated for the test period to assess predictive performance and temporal generalization.\

Predicted values were back-transformed from logarithmic scale (`exp(x) − 1`) to their original units, enabling direct comparison with observed theft cases.\

Performance metrics—including ME, RMSE, MAE, MPE, MAPE, MASE, and R²—were calculated to comprehensively evaluate forecast accuracy.\

Visualization through quarterly step plots further illustrated the degree of alignment between predicted and observed trends, serving as both a model validation and a reproducibility demonstration.

```{r}
library(ggplot2)
library(lubridate)

# Register system font mapping
windowsFonts(
  Times = windowsFont("Times New Roman")
)

# Prediction and back-transformation 
test_data$predicted_log_cases <- predict(log_log_model_11, newdata = test_data)
test_data$predicted_cases <- exp(test_data$predicted_log_cases) - 1
test_data$actual_cases <- test_data$cases  

# Visualization: Predicted vs. Actual Cases (Quarterly) 
ggplot(test_data, aes(x = date)) +
  geom_step(aes(y = actual_cases, color = "Actual Cases"), size = 1.2) +
  geom_step(aes(y = predicted_cases, color = "Predicted Cases"), size = 1.2) +
  scale_color_manual(
    values = c("Actual Cases" = "black",
               "Predicted Cases" = "#00BA38")
  ) +
  scale_x_date(
    date_breaks = "3 months",
    labels = function(x) {
      paste0(format(x, "%Y"), "-Q", lubridate::quarter(x))
    }
  ) +
  scale_y_continuous(
    limits = c(0, 100),
    breaks = seq(0, 100, by = 20)
  ) +
  labs(
    title = "Comparison of Predicted vs. Actual Cases by Quarter",
    x = "Quarter",
    y = "Number of Theft Cases",
    color = "Legend"
  ) +
  theme_minimal() +
  theme(
    text = element_text(family = "Times"),  
    plot.title = element_text(size = 14, face = "bold"),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 10)
  )

# Prediction error evaluation 

# Predict values using the final log-log model
predicted_values <- predict(log_log_model_11, newdata = test_data)

# Back-transform predicted and actual values
predicted_cases <- exp(predicted_values) - 1
test_data$actual_cases <- exp(test_data$log_cases) - 1

# Create result dataframe
prediction_results <- data.frame(
  date = test_data$date,
  actual_cases = test_data$actual_cases,
  predicted_cases = predicted_cases
)

# Compute residuals
errors <- prediction_results$predicted_cases - prediction_results$actual_cases

# Evaluation metrics
ME <- mean(errors)
RMSE <- sqrt(mean(errors^2))
MAE <- mean(abs(errors))
MPE <- mean((errors / prediction_results$actual_cases) * 100)
MAPE <- mean(abs(errors / prediction_results$actual_cases) * 100)

# Naive benchmark for MASE
naive_forecast <- c(NA, head(prediction_results$actual_cases, -1))
naive_errors <- naive_forecast - prediction_results$actual_cases
MASE <- mean(abs(errors)) / mean(abs(naive_errors), na.rm = TRUE)

# R-squared
SSE <- sum((prediction_results$actual_cases - prediction_results$predicted_cases)^2)
SST <- sum((prediction_results$actual_cases - mean(prediction_results$actual_cases))^2)
R_squared <- 1 - SSE / SST

# Output all metrics
cat("Mean Error (ME):", ME, "\n")
cat("Root Mean Squared Error (RMSE):", RMSE, "\n")
cat("Mean Absolute Error (MAE):", MAE, "\n")
cat("Mean Percentage Error (MPE):", MPE, "%\n")
cat("Mean Absolute Percentage Error (MAPE):", MAPE, "%\n")
cat("Mean Absolute Scaled Error (MASE):", MASE, "\n")
cat("R-squared:", round(R_squared, 4), "\n")
```

## Newey-west

To address potential autocorrelation and heteroskedasticity that may persist in time series regression, Newey–West robust standard errors were computed under multiple bandwidths (4–24).\

This correction ensures that coefficient significance remains consistent even when traditional OLS assumptions are relaxed, thereby reinforcing the robustness of inference.\

By presenting results under different bandwidths, the study transparently demonstrates that key relationships remain statistically reliable under alternative specifications.

```{r}
library(lmtest)
library(sandwich)

# Newey–West covariance matrix (robustness check under varying bandwidths)
bandwidths <- c(4, 6, 12, 18, 24)

for (L in bandwidths) {
  cat("\n--- Bandwidth =", L, "---\n")
  nw_cov <- NeweyWest(log_log_model_11, lag = L, prewhite = FALSE, adjust = TRUE)
  print(coeftest(log_log_model_11, vcov. = nw_cov))
}
```

Last updated: 23 July 2025 Maintainer: \[Hongbo Zhao / Contact: hongbo.zhao\@uqconnect.edu.au\]\
\
