---
title: "Untitled"
format: html
editor: visual
---

## XGBoost Specification and Reproducibility Protocol

To ensure the reproducibility of our XGBoost-based modeling framework, all procedures—including data preprocessing, model training, evaluation, and variable importance analysis—were fully scripted in R and conducted under a fixed random seed (`set.seed(123)`). The workflow was designed to systematically track each modeling decision, ensuring that future researchers can replicate and validate the results.

## Data Preparation and Feature Engineering

Two sets of feature matrices were constructed for both the training and testing datasets. One version used raw values, while the other adopted log-transformed predictors to accommodate skewness and stabilize variance. Variables included lagged economic indicators (e.g., copper prices, unemployment rate), scrap metal imports, seasonal dummies, and intervention variables. Feature matrices were generated using `model.matrix()` to guarantee design matrix consistency across different folds and experiments.

## Hyperparameter Tuning via Grid Search and Cross-Validation

We implemented a comprehensive hyperparameter grid search over combinations of `max_depth`, `eta`, `subsample`, and `colsample_bytree`, with five-fold cross-validation. The objective was set to `"reg:squarederror"`, and the evaluation metric was root mean squared error (RMSE). To avoid overfitting and optimize generalization, `early_stopping_rounds = 10` was applied. The optimal parameter set and the corresponding number of boosting rounds were determined based on the lowest mean test RMSE across folds.

```{r}
options(repos = c(CRAN = "https://cloud.r-project.org"))
```

```{r}
# 1. Construct the feature matrix for training and testing sets (raw scale, without log-transformation)
X_train_1 <- model.matrix(~ Total_theft.lag + Scrap_Imports_CN.lag + Copper_price.lag + Unwork_NSW.lag + intervention.lag + t_since.lag + Feb + Mar + APr + May + Jun + Jul + Aug + Sep + Oct + Nov + Dec,
                          data = train_data)

X_test_1 <- model.matrix(~ Total_theft.lag + Scrap_Imports_CN.lag + Copper_price.lag + Unwork_NSW.lag + intervention.lag + t_since.lag + Feb + Mar + APr + May + Jun + Jul + Aug + Sep + Oct + Nov + Dec,
                         data = test_data)

# 2. Create DMatrix objects for training and testing
dtrain <- xgb.DMatrix(data = X_train_1, label = train_data$cases)
dtest <- xgb.DMatrix(data = X_test_1)

# 3. Define the hyperparameter search grid
grid <- expand.grid(
  max_depth = c(3, 5, 7),
  eta = c(0.01, 0.05, 0.1),
  subsample = c(0.6, 0.8),
  colsample_bytree = c(0.6, 0.8)
)

# 4. Perform grid search with 5-fold cross-validation (using test RMSE as the evaluation metric)
best_rmse <- Inf
best_param <- list()
best_nrounds <- 0

for (i in 1:nrow(grid)) {
  param <- list(
    objective = "reg:squarederror",
    eval_metric = "rmse",
    max_depth = grid$max_depth[i],
    eta = grid$eta[i],
    subsample = grid$subsample[i],
    colsample_bytree = grid$colsample_bytree[i]
  )
  
  set.seed(123)  # Ensures reproducibility across CV iterations
  cv_result <- xgb.cv(
    params = param,
    data = dtrain,
    nrounds = 300,
    nfold = 5,
    early_stopping_rounds = 10,
    verbose = 0
  )
  
  if (cv_result$evaluation_log$test_rmse_mean[cv_result$best_iteration] < best_rmse) {
    best_rmse <- cv_result$evaluation_log$test_rmse_mean[cv_result$best_iteration]
    best_param <- param
    best_nrounds <- cv_result$best_iteration
  }
}

# 5. Display the optimal hyperparameters and number of boosting rounds
print("Best parameters:")
print(best_param)
print(paste("Best nrounds:", best_nrounds))

# 6. Train the final XGBoost model using optimal settings
xgb_final <- xgb.train(
  params = best_param,
  data = dtrain,
  nrounds = best_nrounds,
  verbose = 1
)

# 7. Generate predictions on the test set
xgb_pred <- predict(xgb_final, dtest)

# 8. Compute RMSE as a primary evaluation metric
rmse <- sqrt(mean((xgb_pred - test_data$cases)^2))
print(paste("Test RMSE:", round(rmse, 2)))

# Assign XGBoost predictions as final output
final_predictions_xgb <- xgb_pred

# 9. Compute residuals (in original scale)
final_residuals_xgb <- test_data$cases - final_predictions_xgb

# 10. Residual diagnostics: Autocorrelation analysis via ACF & PACF
acf(final_residuals_xgb, main = "ACF of XGBoost Residuals")
pacf(final_residuals_xgb, main = "PACF of XGBoost Residuals")

# 11. Test for residual autocorrelation using the Ljung–Box test
box_ljung_test_xgb <- Box.test(final_residuals_xgb, lag = 20, type = "Ljung-Box")
print(box_ljung_test_xgb)

# 12. Prepare data frame for export or visualization
export_data_xgb <- data.frame(
  date = test_data$date,
  actual_cases = test_data$cases,
  predicted_cases = final_predictions_xgb
)

# 13. Compute MAPE on the test set
mape_xgb <- mean(abs((test_data$cases - final_predictions_xgb) / test_data$cases)) * 100
cat("MAPE of XGBoost on the test set:", round(mape_xgb, 2), "%\n")

# 14. [Optional] Save predictions to CSV for external reporting
# write.csv(export_data_xgb, "xgboost_predictions.csv", row.names = FALSE)

# 15. Compute additional error metrics (RMSE, MAE, MPE, R-squared)
rmse_xgb <- sqrt(mean((test_data$cases - final_predictions_xgb)^2))
mae_xgb <- mean(abs(test_data$cases - final_predictions_xgb))
mpe_xgb <- mean((test_data$cases - final_predictions_xgb) / test_data$cases) * 100

# Total Sum of Squares and Residual Sum of Squares for R²
sst_xgb <- sum((test_data$cases - mean(test_data$cases))^2)
sse_xgb <- sum((test_data$cases - final_predictions_xgb)^2)
r_squared_xgb <- 1 - sse_xgb / sst_xgb

# Print all performance metrics
cat("RMSE:", round(rmse_xgb, 2), "\n")
cat("MAE:", round(mae_xgb, 2), "\n")
cat("MPE:", round(mpe_xgb, 2), "%\n")
cat("R-squared:", round(r_squared_xgb, 4), "\n")
```

```{r}
# 1. Construct the full feature matrix for training and testing sets (log-transformed variables)
X_train <- model.matrix(~ log_Total_theft.lag + log_Copper_price.lag + log_Scrap_Imports_CN.lag + 
                          log_Unwork_NSW.lag + intervention.lag + log_t_since.lag + 
                          Feb + Mar + APr + May + Jun + Jul + Aug + Sep + Oct + Nov + Dec,
                        data = train_data)

X_test <- model.matrix(~ log_Total_theft.lag + log_Copper_price.lag + log_Scrap_Imports_CN.lag + 
                         log_Unwork_NSW.lag + intervention.lag + log_t_since.lag + 
                         Feb + Mar + APr + May + Jun + Jul + Aug + Sep + Oct + Nov + Dec,
                       data = test_data)

# 2. Create DMatrix objects required by the XGBoost engine
dtrain <- xgb.DMatrix(data = X_train, label = train_data$log_cases)
dtest <- xgb.DMatrix(data = X_test)

# 3. Define a grid of hyperparameters for model tuning
grid <- expand.grid(
  max_depth = c(3, 5, 7),
  eta = c(0.01, 0.05, 0.1),
  subsample = c(0.6, 0.8),
  colsample_bytree = c(0.6, 0.8)
)

# 4. Conduct grid search with 5-fold cross-validation (minimizing test RMSE)
best_rmse <- Inf
best_param <- list()
best_nrounds <- 0

for (i in 1:nrow(grid)) {
  param <- list(
    objective = "reg:squarederror",
    eval_metric = "rmse",
    max_depth = grid$max_depth[i],
    eta = grid$eta[i],
    subsample = grid$subsample[i],
    colsample_bytree = grid$colsample_bytree[i]
  )
  
  set.seed(123)  # Ensure reproducibility
  cv_result <- xgb.cv(
    params = param,
    data = dtrain,
    nrounds = 300,
    nfold = 5,
    early_stopping_rounds = 10,
    verbose = 0
  )
  
  if (cv_result$evaluation_log$test_rmse_mean[cv_result$best_iteration] < best_rmse) {
    best_rmse <- cv_result$evaluation_log$test_rmse_mean[cv_result$best_iteration]
    best_param <- param
    best_nrounds <- cv_result$best_iteration
  }
}

# 5. Output the best hyperparameter configuration and optimal number of boosting rounds
print("Best parameters:")
print(best_param)
print(paste("Best nrounds:", best_nrounds))

# 6. Train the final XGBoost model with the selected parameters
xgb_final <- xgb.train(
  params = best_param,
  data = dtrain,
  nrounds = best_nrounds,
  verbose = 1
)

# 7. Generate predictions on the test set (log scale)
xgb_pred <- predict(xgb_final, dtest)

# 8. (Optional) Calculate RMSE on log scale for reference
rmse <- sqrt(mean((xgb_pred - test_data$log_cases)^2))
print(paste("Test RMSE (log scale):", round(rmse, 2)))

# 9. Back-transform predictions to original scale (count level)
final_predictions_xgb <- exp(xgb_pred) - 1
actual_values <- test_data$cases

# 10. Compute residuals on original scale
final_residuals_xgb <- actual_values - final_predictions_xgb

# 11. Residual autocorrelation test (Ljung–Box)
box_ljung_test_xgb <- Box.test(final_residuals_xgb, lag = 20, type = "Ljung-Box")
print(box_ljung_test_xgb)

# 12. Compute primary performance metrics on the original scale
mape_xgb <- mean(abs(final_residuals_xgb / actual_values)) * 100        # Mean Absolute Percentage Error
mae_xgb <- mean(abs(final_residuals_xgb))                                # Mean Absolute Error
mpe_xgb <- mean(final_residuals_xgb / actual_values) * 100               # Mean Percentage Error
rmse_xgb <- sqrt(mean(final_residuals_xgb^2))                            # Root Mean Squared Error

# 13. Compute R-squared (coefficient of determination)
sst_xgb <- sum((actual_values - mean(actual_values))^2)                 # Total Sum of Squares
sse_xgb <- sum((final_residuals_xgb)^2)                                 # Residual Sum of Squares
r_squared_xgb <- 1 - sse_xgb / sst_xgb

# 14. Print performance summary
cat("MAPE of XGBoost on the test set:", round(mape_xgb, 2), "%\n")
cat("RMSE:", round(rmse_xgb, 2), "\n")
cat("MAE:", round(mae_xgb, 2), "\n")
cat("MPE:", round(mpe_xgb, 2), "%\n")
cat("R-squared:", round(r_squared_xgb, 4), "\n")
```

The empirical results strongly supported the superiority of the log-transformed model. As shown in the performance outputs (see Figure X), the untransformed model produced a test MAPE of 28.81%, an RMSE of 14.75, and an R-squared of only 0.0344, indicating weak explanatory power and suboptimal generalization. In contrast, the log-transformed specification reduced the test MAPE to 21.89% and the RMSE to 13.07, while substantially increasing R-squared to 0.2418. The mean percentage error (MPE) also improved from –14.69% to 0.21%, suggesting that the transformed model yielded more directionally accurate forecasts.

Furthermore, residual diagnostics from the log-transformed model revealed no significant autocorrelation (Ljung–Box p-value = 0.7543), indicating well-behaved residuals and improved temporal independence. Given these improvements in both predictive accuracy and residual behavior, the log-transformed XGBoost model was selected as the final modeling framework. This transformation not only enhanced the numerical stability and interpretability of the model but also aligns with best practices in statistical learning for count-based or right-skewed response variables.

## Robustness Check via Variable Ablation

To examine the robustness and dependency of model performance on specific predictors, we iteratively removed key variables (e.g., copper prices, theft rate, unemployment, policy intervention) and retrained the model under identical procedures. Each ablation experiment reported performance metrics using the same evaluation framework. The comparative results were visualized as a summary table (Figure X), highlighting the marginal contribution of each predictor to the overall predictive performance.

```{r}
run_xgb_model <- function(formula_str, model_name, train_data, test_data) {
  # 1. Construct feature matrices from model formula
  X_train <- model.matrix(as.formula(formula_str), data = train_data)
  X_test  <- model.matrix(as.formula(formula_str), data = test_data)
  
  dtrain <- xgb.DMatrix(data = X_train, label = train_data$log_cases)
  dtest <- xgb.DMatrix(data = X_test)

  # 2. Perform grid search with 5-fold cross-validation to tune hyperparameters
  grid <- expand.grid(
    max_depth = c(3, 5, 7),
    eta = c(0.01, 0.05, 0.1),
    subsample = c(0.6, 0.8),
    colsample_bytree = c(0.6, 0.8)
  )
  
  best_rmse <- Inf
  for (i in 1:nrow(grid)) {
    param <- list(
      objective = "reg:squarederror",
      eval_metric = "rmse",
      max_depth = grid$max_depth[i],
      eta = grid$eta[i],
      subsample = grid$subsample[i],
      colsample_bytree = grid$colsample_bytree[i]
    )
    
    set.seed(123)  # Ensures reproducibility of CV results
    cv_result <- xgb.cv(
      params = param,
      data = dtrain,
      nrounds = 300,
      nfold = 5,
      early_stopping_rounds = 10,
      verbose = 0
    )
    
    # Update best parameter set if current configuration yields lower RMSE
    if (cv_result$evaluation_log$test_rmse_mean[cv_result$best_iteration] < best_rmse) {
      best_rmse <- cv_result$evaluation_log$test_rmse_mean[cv_result$best_iteration]
      best_param <- param
      best_nrounds <- cv_result$best_iteration
    }
  }
  
  # 3. Train final model using optimal hyperparameters
  model <- xgb.train(params = best_param, data = dtrain, nrounds = best_nrounds, verbose = 0)
  xgb_pred <- predict(model, dtest)
  pred_cases <- exp(xgb_pred) - 1  # Back-transform predictions to original count scale
  
  # 4. Evaluate model performance using standard accuracy metrics (on original scale)
  actual <- test_data$cases
  mape <- mean(abs((actual - pred_cases) / actual)) * 100      # Mean Absolute Percentage Error
  rmse <- sqrt(mean((actual - pred_cases)^2))                  # Root Mean Squared Error
  mae <- mean(abs(actual - pred_cases))                        # Mean Absolute Error
  mpe <- mean((actual - pred_cases) / actual) * 100            # Mean Percentage Error
  r2 <- 1 - sum((actual - pred_cases)^2) / sum((actual - mean(actual))^2)  # R-squared
  
  # Return a summary row of performance metrics for the current model specification
  data.frame(
    Model = model_name,
    MAPE = round(mape, 2),
    RMSE = round(rmse, 2),
    MAE = round(mae, 2),
    MPE = round(mpe, 2),
    R_squared = round(r2, 4)
  )
}
```

```{r}
# Save results from all model specifications
results <- list()

# Full variable model (baseline)
results[["All"]] <- run_xgb_model(
  "~ log_Total_theft.lag + log_Copper_price.lag +
     log_Scrap_Imports_CN.lag + log_Unwork_NSW.lag + intervention.lag +
     log_t_since.lag + Feb + Mar + APr + May + Jun + Jul + Aug + Sep + Oct + Nov + Dec",
  "All",
  train_data, test_data
)

# Remove Copper Price
results[["No_Copper"]] <- run_xgb_model(
  "~ log_Total_theft.lag + log_Scrap_Imports_CN.lag +
     log_Unwork_NSW.lag + intervention.lag + log_t_since.lag +
     Feb + Mar + APr + May + Jun + Jul + Aug + Sep + Oct + Nov + Dec",
  "No_Copper",
  train_data, test_data
)

# Remove Total Theft
results[["No_Theft"]] <- run_xgb_model(
  "~ log_Copper_price.lag +
     log_Scrap_Imports_CN.lag + log_Unwork_NSW.lag + intervention.lag +
     log_t_since.lag + Feb + Mar + APr + May + Jun + Jul + Aug + Sep + Oct + Nov + Dec",
  "No_Theft",
  train_data, test_data
)

# Remove Time Since Intervention
results[["No_t_since"]] <- run_xgb_model(
  "~ log_Total_theft.lag + log_Copper_price.lag +
     log_Scrap_Imports_CN.lag + log_Unwork_NSW.lag + intervention.lag +
     Feb + Mar + APr + May + Jun + Jul + Aug + Sep + Oct + Nov + Dec",
  "No_t_since",
  train_data, test_data
)

# Remove Policy Intervention Indicator
results[["No_intervention"]] <- run_xgb_model(
  "~ log_Total_theft.lag + log_Copper_price.lag +
     log_Scrap_Imports_CN.lag + log_Unwork_NSW.lag +
     log_t_since.lag + Feb + Mar + APr + May + Jun + Jul + Aug + Sep + Oct + Nov + Dec",
  "No_intervention",
  train_data, test_data
)

# Remove Scrap Imports from China
results[["No_Scrap"]] <- run_xgb_model(
  "~ log_Total_theft.lag + log_Copper_price.lag +
     log_Unwork_NSW.lag + intervention.lag +
     log_t_since.lag + Feb + Mar + APr + May + Jun + Jul + Aug + Sep + Oct + Nov + Dec",
  "No_Scrap",
  train_data, test_data
)

# Remove NSW Unemployment Rate
results[["No_Unwork"]] <- run_xgb_model(
  "~ log_Total_theft.lag + log_Copper_price.lag +
     log_Scrap_Imports_CN.lag + intervention.lag +
     log_t_since.lag + Feb + Mar + APr + May + Jun + Jul + Aug + Sep + Oct + Nov + Dec",
  "No_Unwork",
  train_data, test_data
)

# Combine all model results into a single data frame
final_df <- do.call(rbind, results)
print(final_df)

# === Step 1: Load required packages ===
library(gridExtra)
library(ggplot2)
library(cowplot)
library(grid)
library(gtable)
library(dplyr)

# === Step 2: Format the result table (round all numeric values to 4 decimal places) ===
final_df_formatted <- final_df %>%
  mutate_if(is.numeric, ~ round(., 4)) %>%
  mutate(Model = rownames(final_df)) %>%
  select(Model, everything())

# === Step 3: Generate table object using classic formatting ===
clean_table <- ggtexttable(
  final_df_formatted,
  rows = NULL,
  theme = ttheme("classic", base_size = 12)
)

# === Step 4: Create a title for the figure ===
title_text <- text_grob(
  "Figure X. Performance Impact of Variable Removal in the XGBoost Model",
  face = "bold",
  size = 14
)

# === Step 5: Combine the title and table into a single layout ===
combined_plot <- plot_grid(
  title_text,
  clean_table,
  ncol = 1,
  rel_heights = c(0.15, 1)
)

# === Step 6: Display the final figure ===
print(combined_plot)
```

# Final Model Specification and Diagnostic Evaluation

Following the variable ablation experiments, no single predictor was identified as redundant, as each log-transformed feature contributed meaningfully to overall model performance. Consequently, the final XGBoost model retained all log-transformed variables for training. To ensure statistical robustness, a series of diagnostic checks were conducted on the residuals of the final model, including tests for autocorrelation (Ljung–Box), normality (Shapiro–Wilk), and heteroscedasticity (Breusch–Pagan). In addition, multiple performance metrics (e.g., MAPE, RMSE, MAE, R²) were computed to assess predictive accuracy. The alignment between predicted and observed values was visualized through time series comparison plots, and SHAP-based interpretation was applied to quantify the relative contribution of each predictor. These steps jointly validated the final model’s adequacy in capturing the underlying structure of cable theft incidents.

```{r}
# Back-transform XGBoost predictions from log scale to original count scale
final_predictions_xgb <- exp(xgb_pred) - 1  # Inverse transformation: exp(ŷ) - 1

# 1. Compute residuals on the original scale
final_residuals_xgb <- test_data$cases - final_predictions_xgb

# 2. Ljung–Box test for autocorrelation in residuals
box_ljung_test_xgb <- Box.test(final_residuals_xgb, lag = 20, type = "Ljung-Box")
print(box_ljung_test_xgb)

# 3. Shapiro–Wilk test for normality of residuals
shapiro_test_xgb <- shapiro.test(final_residuals_xgb)
print(shapiro_test_xgb)

# 4. Visual inspection of residual distribution via Q-Q plot
qqnorm(final_residuals_xgb,
       main = "Normal Q-Q Plot of XGBoost Residuals")
qqline(final_residuals_xgb, col = "red", lwd = 2)

# 5. Load necessary package for heteroscedasticity test
if (!require(lmtest)) install.packages("lmtest")
library(lmtest)

# 6. Breusch–Pagan test for heteroscedasticity
# Fit a linear model of squared residuals against predicted values
bp_model <- lm(final_residuals_xgb^2 ~ final_predictions_xgb)
bptest_result <- bptest(bp_model)
print(bptest_result)
```

Using the optimal hyperparameters and the selected number of boosting iterations, a final model was trained on the full training dataset. Predictions were then made on the test set, and the results were inverse-transformed (i.e., exponentiated and adjusted) to recover the original scale of theft incidents. Performance was assessed using a suite of standard metrics:

-   Root Mean Squared Error (RMSE)

-   Mean Absolute Error (MAE)

-   Mean Absolute Percentage Error (MAPE)

-   Mean Percentage Error (MPE)

-   R-squared (R²)

The residuals were further examined through ACF/PACF plots and the Ljung–Box test to assess temporal autocorrelation.

```{r}
# Since no variables were excluded from the final model, the previously computed performance metrics are reported below
cat("Performance Metrics of XGBoost Model on Test Set:\n")
cat("--------------------------------------------------\n")
cat("MAPE:       ", round(mape_xgb, 2), "%\n")
cat("RMSE:       ", round(rmse_xgb, 2), "\n")
cat("MAE:        ", round(mae_xgb, 2), "\n")
cat("MPE:        ", round(mpe_xgb, 2), "%\n")
cat("R-squared:  ", round(r_squared_xgb, 4), "\n")
```

```{r}
# === Load required packages for residual diagnostics ===
if (!require("forecast")) install.packages("forecast")
if (!require("ggplot2")) install.packages("ggplot2")
library(forecast)
library(ggplot2)

# Recompute residuals on the original scale (post back-transformation)
final_predictions_xgb <- exp(xgb_pred) - 1
final_residuals_xgb <- test_data$cases - final_predictions_xgb

# Plot ACF of residuals using ggplot2-compatible forecast::ggAcf
acf_plot <- ggAcf(final_residuals_xgb, lag.max = 13) +
  labs(
    title = "ACF of XGBoost Residuals",
    x = "Lag (Months)",
    y = "Autocorrelation"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 14),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold"),
    axis.text = element_text(color = "black"),
    panel.grid.minor = element_blank(),
    plot.background = element_rect(fill = "white", color = NA),
    panel.background = element_rect(fill = "white", color = NA)
  )

# Plot PACF of residuals
pacf_plot <- ggPacf(final_residuals_xgb, lag.max = 13) +
  labs(
    title = "PACF of XGBoost Residuals",
    x = "Lag (Months)",
    y = "Partial Autocorrelation"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 14),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold"),
    axis.text = element_text(color = "black"),
    panel.grid.minor = element_blank(),
    plot.background = element_rect(fill = "white", color = NA),
    panel.background = element_rect(fill = "white", color = NA)
  )

# Display ACF and PACF plots
print(acf_plot)
print(pacf_plot)
```

## Variable Contribution and Model Interpretability

To enhance model transparency, SHAP (SHapley Additive exPlanations) values were computed using the `shapviz` package with `predcontrib = TRUE`. This technique decomposes each prediction into additive contributions of individual features, offering a consistent interpretation across observations. The mean absolute SHAP values were summarized and visualized to identify the most influential predictors.

```{r}
# === Load packages for SHAP value interpretation ===
if (!require("shapviz")) install.packages("shapviz")
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("dplyr")) install.packages("dplyr")

library(shapviz)
library(ggplot2)
library(dplyr)

# === 1. Compute SHAP values (XGBoost requires predcontrib = TRUE) ===
shap_values_train <- predict(xgb_final, dtrain, predcontrib = TRUE)

# === 2. Remove the BIAS term (model intercept; not a feature) ===
shap_values_train_fixed <- shap_values_train[, !colnames(shap_values_train) %in% "BIAS", drop = FALSE]

# === 3. Dimension check (ensure SHAP values match input matrix) ===
print(dim(shap_values_train_fixed))  # Expected: (n_samples, n_features)
print(dim(X_train))

# === 4. Create SHAP object for visualization (optional: interactive use with shapviz tools) ===
shap_obj_train <- shapviz(shap_values_train_fixed, X = X_train)

# === 5. Calculate average absolute SHAP values and convert to percentage contribution ===
shap_means <- colMeans(abs(shap_values_train_fixed))
shap_pct <- shap_means / sum(shap_means) * 100

# === 6. Create data frame of SHAP summary statistics ===
shap_df <- data.frame(
  Feature = names(shap_means),
  Mean_SHAP_Value = shap_means,
  Importance_Percentage = shap_pct
) %>%
  arrange(desc(Importance_Percentage))  # Sort by importance

print(shap_df)  # View feature ranking and relative importance
# === 7. Prepare data for ggplot2 bar chart (sorted from least to most important) ===
shap_plot_df <- shap_df %>%
  arrange(Importance_Percentage) %>%
  mutate(Feature = factor(Feature, levels = Feature))  # Control feature order for plotting

# === 8. Visualize SHAP feature importance as a horizontal bar plot (percentage scale) ===
shap_plot <- ggplot(shap_plot_df, aes(x = Importance_Percentage, y = Feature)) +
  geom_bar(stat = "identity", fill = "#1F77B4", width = 0.6) +
  labs(
    title = "SHAP Feature Importance (Percentage View)",
    x = "Relative Importance (%)",
    y = "Predictor Variables"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 14),
    axis.title.x = element_text(face = "bold", size = 12),
    axis.title.y = element_text(face = "bold", size = 12),
    axis.text = element_text(color = "black", size = 10),
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    plot.background = element_rect(fill = "white", color = NA),
    panel.background = element_rect(fill = "white", color = NA)
  )

# Display SHAP importance plot
print(shap_plot)
```

## Visualization and Result Export

All predicted vs. actual case trajectories were visualized using `ggplot2`, with quarter-based time axes and color-coded step plots. In addition, both the predicted values and residuals were exported into structured data frames for post-hoc analysis or replication. The residual diagnostics and SHAP-based feature rankings ensure that both the predictive accuracy and interpretability of the XGBoost model are reproducibly documented.

```{r}
library(ggplot2)
library(lubridate)
library(dplyr)

# Construct a data frame for plotting: actual vs. predicted values
plot_data_xgb <- test_data %>%
  mutate(
    predicted_cases = final_predictions_xgb,
    actual_cases = cases
  )

# Generate step plot comparing XGBoost predictions with observed values
xgb_plot <- ggplot(plot_data_xgb, aes(x = date)) +
  geom_step(aes(y = actual_cases, color = "Actual Cases"), size = 1.2) +
  geom_step(aes(y = predicted_cases, color = "Predicted Cases"), size = 1.2) +
  scale_color_manual(
    values = c("Actual Cases" = "black",
               "Predicted Cases" = "#D62728")  # Red for predicted
  ) +
  scale_x_date(
    date_breaks = "3 months",
    labels = function(x) paste0(format(x, "%Y"), "-Q", quarter(x))  # Quarterly labels
  ) +
  scale_y_continuous(
    limits = c(0, 100),
    breaks = seq(0, 100, by = 20)
  ) +
  labs(
    title = "Comparison of XGBoost-Predicted vs. Actual Cases by Quarter",
    x = "Quarter",
    y = "Number of Theft Cases",
    color = "Legend"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 10),
    plot.background = element_rect(fill = "white", color = NA),
    panel.background = element_rect(fill = "white", color = NA)
  )

# Render the final plot
print(xgb_plot)
```

Last updated: 23 July 2025 Maintainer: \[Hongbo Zhao / Contact: hongbo.zhao\@uqconnect.edu.au\]
